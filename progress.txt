This is a simple file use to store the progess and steps made for this web service.

First application I will work towards is the authorization step. Users will create an account
or sign in. Creating an account will prompt the user with multiple things that will be stored in the users db.
1. Username
2. Password (I want to do hash but I dont rly remember how to do so)
3. Sizes (This is the entire premise of the application so I will ask users some basic sizes)
    a. Top size (range)
    b. Pants waist (range)
    c. Pants length (range)
    d. Shoe size
    This will be all for now
4. Gender they will be shopping for

First I will create a db that can store all this information and call it users.

Now that I have created the db alongside the read/read-write user access, my next step
Is to create a client file where I can call certain lambda functions

I have created the basics of the client to be prompted, I now will work on the first lambda function
Which is the authorization function. This will allow a user to log in and return a token that will
eventually expire but until it does gives users the ability to check out the catalog.

I refurbished the old authorize lambda function that was given to us by joseph hummel to fit with my db.
I do feel like I could have put more effort into that, but thats not the beauty of my project.
The beauty is the sql db calls alongside the webscrapping lambda function that I am scared to make.

Now I will be working on a making account feature, the premise is very simple.
Function will prompt user with a few questions and limited answers that they user can pick from.

I finally was able to implement creating a user and validting the inputs that they provided.
its 10:30 on a saturday im gonna kms, bed time now. Tmrw I will work on editing user stats, which will
use this helper function for sure
"
VALID_TOP_SIZES = {'XXS', 'XS', 'S', 'M', 'L', 'XL', 'XXL', '3XL'}
VALID_GENDERS = {'M', 'F', 'Other'}
MIN_PANTS_WAIST, MAX_PANTS_WAIST = 24, 50
MIN_PANTS_LENGTH, MAX_PANTS_LENGTH = 26, 40
MIN_SHOE_SIZE, MAX_SHOE_SIZE = 4.0, 14.0  # Shoe size is a float

REQUIRED_FIELDS = {"username", "password", "top_size", "pants_waist", "pants_length", "shoe_size", "gender"}

def validate_user_input(body):
    # Ensure all required parameters are present
    missing_fields = REQUIRED_FIELDS - body.keys()
    if missing_fields:
        return api_utils.error(400, f"Missing parameters: {', '.join(missing_fields)}")

    # Validate top_size
    if body["top_size"] not in VALID_TOP_SIZES:
        return api_utils.error(400, f"Invalid top size: {body['top_size']}")

    # Validate numeric fields
    try:
        pants_waist = int(body["pants_waist"])
        pants_length = int(body["pants_length"])
        shoe_size = float(body["shoe_size"])
    except ValueError:
        return api_utils.error(400, "One or more numeric fields are not valid numbers")

    # Range checks
    if not (MIN_PANTS_WAIST <= pants_waist <= MAX_PANTS_WAIST):
        return api_utils.error(400, f"Invalid pants waist: {pants_waist}")
    if not (MIN_PANTS_LENGTH <= pants_length <= MAX_PANTS_LENGTH):
        return api_utils.error(400, f"Invalid pants length: {pants_length}")
    if not (MIN_SHOE_SIZE <= shoe_size <= MAX_SHOE_SIZE and shoe_size * 2 == round(shoe_size * 2)):
        return api_utils.error(400, f"Invalid shoe size: {shoe_size}")

    # Validate gender
    if body["gender"] not in VALID_GENDERS:
        return api_utils.error(400, f"Invalid gender: {body['gender']}")

    return None  # No errors
"

and also attempt to dive a bit in the scrapping, I have all week to do this, so hopefully I can do this.
night time.

Time to start the work week, it is monday, not morning but rather 4:14 pm... Anyways, it came to my knowledge 
earlier today that there already exists a webscrapping API for the first website I was planning on scrapping ASOS.
Now I am pitched the dilemna of either using this API and paying the fee of however much its costs. Actually let 
me take a look at how expensive a key is. So I have multiple options
    1. I can use Puppeteer which is a js library that provides a high level api to control Chrome or Firefox over the DevTools 
        Protocol or WebDriver BiDi. Puppeteer also runs headless meaning there is no visible UI by default. I think I can utilize 
        this I would have two days to learn the library before I start getting behind on my design. (Update) after looking at a few reviews,
        I dont think I will continue with puppeteer because it looks like it is a bit of a complicated tool in comparison to some others like
        BeautifulSoup or even more sought after js lib's like Playwright. Although I def do not want to scrape in js, I barely know the lang. 
    2. I can use the Scrapeos ASOS API (lol this is pretty much what I need) which is a very easy solution to my problem, but in turn
        does not allow me to create my own web scrapper which in my eyes is kinda the bread and butter of my final project. I dont think
        my ego nor my frugality is going to allow me to use this, but if I cant learn basic web scrapping by the end of the day, I think
        I will push my ego and cheap nature aside for peace of mind. But I am rly excited to learn how to scrape. (Note) the pricing for this
        Is not horrible, I get 5 free api requests after that I can spend $20 for 100 credits, $80 for 500, or $140 for 1000. I wont need much since 
        im literally just gonna call and store the info in a rds, but i like a lot of trial and error, and i know I definetily will not succeed in less
        than 5 attempts.
    3. BeautifulSoup! Never tried it before but I was reccomended it by a well trusted buddy of mine. I have no clue how I will go 
        about learning this, but what i like is that it is free, not attempt restricted, and in python (yay!). Im gonna try this approach first, 
        and if I fail miserably by the end of the day, maybe revisit my approach. 

Holy crap the scrapper works, and its really simple too. So far it works on any asos website that displays a catalog.
Example of these websites are attached below, its very simple and im really excited that it works!

https://www.asos.com/us/men/sale/cat/?cid=8409&ctaref=hp|mw|sale|carousel|1|category|viewall
(on a side note do not use this first link because it is a 5000 that would take ages to load,
the maximum catalog items it can handle would be 1500 items)
https://www.asos.com/us/men/ctas/fashion-online-13/cat/?cid=13522

For the scrapper I ended up using BeautifulSoup, watched one 10 min youtube video, and applied the theory.
with a little bit of dsa, I was able to store pages of information in dictionary. As of right now I only have
the title and price of the item stored, I will eventually store the link and photo of the items aswell.

Okay so I was able to obtain the link, but I am having trouble with the photo url, i have suspiscion it is due
to lazy production that doesnt allow all the images to be loaded for some reason. Thats fine by me i will obtain the photos
through a seperate lambda function. My next steps are to create another db but this part is going to be a bit difficult.
The db will have the following attributes:
item id(primary key)
title(unique) in order to prevent me from inputting duplicate items
price
item category (whether it is a top, bottom, or shoe)
sizes (idk how to store yet)
colors (idk how to store yet)
gender (M,F,Unisex)
image (ill store this in an s3 or something with the db item id referencing it)

I genuinly have no idea how to store this it kinda has me stressing becase of my ranges
on my users db
top_size ['XXS', 'XS', 'S', 'M', 'L', 'XL', 'XXL', '3XL']
pants_waist [24-50]
pants_length [26-40]
shoe_size [6-15]
gender ['M', 'F', 'Other']

Inside the catalog db will consist of an Items table, Sizes table, colors table, and a photos table that was made
with the code below.
-- 2. Create first table in fitfinder called users
Create Table items(
    itemid  int not null AUTO_INCREMENT, -- primary key for each item
    item_name VARCHAR(64) not null, -- name of item cannot be repeated
    price VARCHAR(16) not null, -- price of each item
    item_gender ENUM('Men', 'Women', 'Unisex') not null, -- Gender of the item restricted to specific values
    PRIMARY KEY (itemid),
    UNIQUE (item_name)
);

ALTER TABLE items AUTO_INCREMENT = 10001; -- Starting value to auto incr with

CREATE table sizes(
    itemid int not null, -- Foreign key to map which item the size is for
    size VARCHAR(128) not null, -- Size of the item
    FOREIGN KEY (itemid) REFERENCES items(itemid)
);

CREATE table colors(
    itemid int not null, -- Foreign key to map which item the color is for
    color VARCHAR(64) not null, -- Color of the item
    photo_url VARCHAR(256) not null, -- URL of the photo of the item in this color
    FOREIGN KEY (itemid) REFERENCES items(itemid)
);

Now the use case of this is to connect an item with all its variants. Lambda scraper function takes a LONG Time
and i Dont want to use polling so im just going to mention that in my readme. It literally took 5 mins to 
obtain 150 items, thats tragic. I will populate it once i get get the client connected to it. Next step is to connect
Client to web scrape (which should take max 30 mins), then after is to display information to client which I 
havent figured out how to do yet. After all this, I will be done, just need to get the documentation looking nicely.

It did not take 30 mins to implement the client, I think thats like the entire situation with my life, thinking itll just take 30 
mins to add a feature to this project and it ending up not doing that. Anyway, I had to switch up my system to use 
amazons SQS to queue a request to a lambda worker-scraper function because my api gateway had too short of a timeout. In laymans terms,
I upload a link to be scraped which talks to a lambda function through api gateway trigger. that lambda function then
adds the link aswell as a job id into amazons SQS for a seperate lambda function to handle. After the message is sent, the server responds
with a taskid number that polls the progress of the website being scrapped. (its kinds weird and not linear at all so thats something)
The information will be inputted into the rds with sizes and colors and everything like that. so my final step now
is to retrive information. first let me clean up commit, clean up, then commit again.